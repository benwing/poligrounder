See `README.textgrounder` for an introduction to TextGrounder and to the
Geolocate subproject.  This file describes how to preprocess raw data from
other sources to generate corpora for use in TextGrounder.

=============
Output format
=============

The normal output format from the various preprocessing applications is
called the "textdb" format.  This is output by (e.g.) ParseTweets.

This format stores data in as a tab-separated database, with one item
per line.  There will be one or more files containing data, ending in
'-SUFFIX.txt' (or '-SUFFIX.txt.bz2' or '-SUFFIX.txt.gz', if the data is
compressed), where SUFFIX is an identifier referring to the particular sort
of data stored. (E.g. tweet data uses the suffix 'tweets'.) In addition, there
is a single file called a "schema", whose name ends in '-SUFFIX-schema.txt'.
This specifies the name of each of the fields, as well as the name and value
of any "fixed" fields (which, conceptually, have the same value for all rows,
and are used to store extra info about the corpus).  Generally, there will
be multiple output files when there are multiple reducers in Hadoop, since
each reducer produces its own output file.  Storing the data this way makes
it easy to read the files from Hadoop.  Large data files are typically
stored compressed, and are automatically uncompressed as they are read in.

The following formats are used for individual fields:

* Integers and floating-point values, output in the obvious way.
* Timestamps, output as long integers specifying milliseconds since the
  Unix Epoch (Jan 1, 1970).
* Lat/long coordinates, output as a comma-separated pair of floating-point
  values.
* Single strings.  URL encoding is used to encode TAB characters (%09 = field
  separator), newlines (%0A = record separator), the percent sign (%25), and
  any other characters that may be used as separators (e.g. greater-than
  signs, spaces, and colons in various cases below).
* Sequences of strings.  The individual strings are URL-encoded and separated
  by >> signs.
* "Count maps", i.e. maps from strings to integers (e.g. counts of words).
  The format is "STRING:COUNT STRING:COUNT ...", where individual strings are
  URL-encoded.  N-grams use the format "WORD1:WORD2:...:COUNT" for an
  individual n-gram. (If additional parameters need to be specified, they
  should be given in a format such as ":PARAM:VALUE", with an initial colon.
  This facility isn't currently used.)

All text should be encoded in UTF-8.

The schema file is currently in the following format:

-- The first line lists each field name separated by a tab.
-- Additional lines specify "fixed fields", which are conceptually like
   normal fields but have the same value for every row.  There is one
   line per fixed field, with the field name, a tab, and the field value.

Possibly at some point it may be necessary to specify the field type as well
as its name.

========================
Downloading Twitter data
========================

There's a script called `twitter-pull` inside of the Twools package
(https://bitbucket.org/utcompling/twools), which can be used for downloading
tweets from the Twitter Streaming API.  This automatically handles retrying
after errors along with exponential backoff (absolutely necessary when doing
Twitter streaming, or you will get locked out temporarily), and starts a new
file after a certain time period (by default, one day), to keep the resulting
BZIP-ped files from getting overly large.

===============
Parsing Twitter
===============

ParseTweets (opennlp.textgrounder.preprocess.ParseTweets) is a preprocessing
application that parses tweets in various ways, optionally filtering and/or
grouping them by user, time, etc.  Numerous options are provided.

ParseTweets can do a lot of things.  By default, it reads in tweets in JSON
format (the raw format from Twitter) and converts them into textdb format
(see above).  However, it can also handle different formats on input
and output, as well as group or filter the tweets.  For example, it is
possible to read in tweets in textdb format, as well as write out tweets in
JSON format when no grouping is done.

When reading tweets in JSON format, duplicate tweets are automatically
filtered out; filtering is by user ID.  This is important because Twitter
often returns duplicate tweets, and it also allows overlapping scrapes of
the same source data to be combined.  However, when reading data in other
formats, no deduplication is done, because user ID's often aren't available
(e.g. when tweets have been grouped).

------ Input and output formats ------

Input and output formats are specified using '--input-format' and
'--output-format', respectively.  Currently, the following formats are handled:

* "textdb" format, on both input and output.
* JSON format, on both input and output, but output in JSON format is not
  possible when tweets are grouped.
* "raw-lines" format on input, treating the text of each line as if it
   were a tweet.  This makes it possible to do n-gram processing and other
   such operations on raw text.
* "stats" format on output.  This outputs statistics on the tweets in
  "textdb" format.

Further control over the fields output in textdb format is possible using
'--output-fields' (specifying which fields to output).  Output of n-grams
is possible using the '--max-ngram' option, specifying the maximum size
of n-grams to output (the default is 1, meaning output only unigrams).

In general, code written using Scoobi should "just work", and require litle
or no more effort to get working than anything else that uses Hadoop.
Specifically, you have to compile the code, build an assembly exactly as you
would do for running other Hadoop code (see above), copy the data into HDFS,
and run.  For example, you can run `TwitterPull.scala` as follows using
the TextGrounder front end:

$ textgrounder --hadoop run opennlp.textgrounder.preprocess.TwitterPull input output

or you can run it directly using `hadoop`:

------ Grouping ------

Grouping is specified using '--grouping' and can be done in the following ways:

* No grouping (the default).
* By user.  When doing this, the location (lat/long coordinates) of the group
  are determined by the earliest tweet in the group that has a location.
  (The 'geo-timestamp' field is used to help in tracking this.)
* By time slice.  The '--timeslice' argument specifies the size of the time
  slices, and all tweets within a given slice are aggregated.
* By file.  All tweets in a given input file are aggregated.  This is
  particularly useful when outputting statistics, or when doing n-gram
  processing on raw text.

------ Filtering ------

When filtering tweets, tweets can be filtered either by the presence of
particular words or word sequences in the text of a tweet or based on the
timestamp of the tweet.  Arbitrary boolean expressions can be specified to
express more complex filters.  Filtering can be done either on the individual
tweet level ('--filter-tweets') or the group level ('--filter-groups'); in
the latter case, a group will be passed through if any tweet in the group
matches the filter.  Matching is normally case-insensitive, but case-sensitive
matching is possible using the '--cfilter-tweets' or '--cfilter-groups'
options.

It is possible to filter by group without actually grouping the tweets
together in the output; this is done by specifying the appropriate type of
grouping using '--grouping', and then specifying '--ungrouped-output'.

=================================
Other apps for processing Twitter
=================================

There's a script `python/run-process-geotext` used to preprocess the GeoText
corpus of Eisenstein et al. (2010) into a textdb corpus.  This is a
front end for `python/twitter_geotext_process.py` and runs similarly to
the Wikipedia preprocessing code below.

============================
Parsing Wikipedia dump files
============================

*NOTE*: Parsing raw Wikipedia dump files is not easy.  Perhaps better
would have been to download and run the MediaWiki software that generates
the HTML that is actually output.  As it is, there may be occasional
errors in processing.  For example, the code that locates the geotagged
coordinate from an article uses various heuristics to locate the coordinate
from various templates which might specify it, and other heuristics to
fetch the correct coordinate if there is more than one.  In some cases,
this will fetch the correct coordinate even if the MediaWiki software
fails to find it (due to slightly incorrect formatting in the article);
but in other cases, it may find a spurious coordinate. (This happens
particularly for articles that mention a coordinate but don't happen to
be themselves tagged with a coordinate, or when two coordinates are
mentioned in an article. FIXME: We make things worse here by picking the
first coordinate and ignoring `display=title`.  See comments in
`get_coord()` about how to fix this.)


------ Quick start ------

Use the following to download a Wikipedia dump and preprocess it to create
a corpus for input to TextGrounder:

$ download-preprocess-wiki WIKITAG

where WIKITAG is something like 'dewiki-20120225', which names an existing
version of Wikipedia on http://dumps.wikipedia.org.  This downloads the
given Wikipedia dump into a subdirectory of the current directory
named WIKITAG, and then preprocesses it into a corpus of the format
needed by TextGrounder.

$ download-preprocess-wiki 'preprocess-dump' to run all of the steps below.
This in turn calls 'run-processwiki' with a series of steps to execute.
This script in turn uses the lower-level script `python/processwiki.py`
to process the actual (unzipped) dump file on stdin according to various
command-line arguments.


------ In detail ------

There are a large number of steps that go into preprocessing a Wikipedia
dump, and a number of different files containing various sorts of
information.  Not all of them actually go into the final corpus normally
used by TextGrounder.  Some of the complexity of this process is due to
the fact that it was created bit-by-bit and evolved over time at the same
time that TextGrounder itself did.

The preprocessing process involves downloading the dump file from
http://dumps.wikipedia.org (which has a name like
enwiki-20120211-pages-articles.xml.bz2), and going through a series of
steps that involve generating various files, terminating with files in the
standard TextGrounder corpus format, which will have names similar to the
following:

1. enwiki-20120211-permuted-dev-unigram-counts.txt (the actual data for the
   documents in the dev set)
2. enwiki-20120211-permuted-dev-unigram-counts-schema.txt (a small "schema"
   file describing the fields contained in the data file, plus certain other
   information)
3. a similar pair of files for the training and test sets.

Note that part of the proess involves permuting (i.e. randomly reordering)
the articles in the dump file, leading to the creation of a new permuted
dump file with a name like enwiki-20120211-permuted-pages-articles.xml.bz2.
(The order of articles in the raw, unpermuted dump file is highly non-random.)
This is done so that the training, dev and test sets can be extracted in
a simple fashion and so that partial results can be reported during the
testing phase, with an expectation that they will reliably reflect the
final results.


From highest to lowest, there are the following levels of scripts:

1. 'download-preprocess-wiki' does everything from start to finish,
   downloading a dump, preprocessing it into various output files, and
   using them to generate a TextGrounder corpus.  It uses 'wget' to
   download the dump; 'preprocess-dump' to process the dump and generate
   data files sufficient to generate a corpus; and 'convert-corpus-to-latest'
   to actually generate the corpus.

2. 'preprocess-dump' takes a raw dump and processes it in various ways in
   order to generate a number of output files.  For historical reasons,
   a TextGrounder corpus isn't directly generated but rather an older
   data format containing equivalent information.  An additional script
   ('convert-corpus-to-latest') is needed to convert this older format
   to the TextGrounder corpus format.  In addition, some extra data files
   are generated that aren't currently used by TextGrounder. (Eventually,
   these issues should be fixed.) 'preprocess-dump' uses some lower-level
   scripts to generate the data files.  Most important among them is
   'run-processwiki'.

3. 'run-processwiki' is a driver script that can be run in various modes
   to generate various sorts of preprocessed data files from the raw
   Wikipedia dump.  It runs the actual preprocessing script (processwiki.py)
   with the appropriate arguments, providing the dump file as standard input.

4. 'processwiki.py' is the actual script that creates the data files.  It
   takes command-line arguments specifying modes to run in and files
   containing ancillary information of various sorts, including information
   previously generated the same script when run in a different mode.  The
   dump file is provided as the standard input of this script rather than
   through a command-line argument, and the script generates its data files
   on its standard output.  'processwiki.py' has no concept of any naming
   convention for the files it generates; it is the job of 'run-processwiki'
   to keep track of the various files.


For 'run-processwiki' and 'preprocess-dump', the dump file used for input
and the various generated files are stored into and read from the current
directory.  'download-preprocess-wiki' creates a subdirectory of the
current directory with the name of the dump being processed (e.g.
'enwiki-20120317').  

When directly running any script below the top-level script
'download-preprocess-wiki', it's strongly suggested that

1) The dump file be made read-only using 'chmod a-w'.
2) A separate directory is created to hold the various generated preprocessed
   data files.  Either the dump file should be moved into this directory
   or a symlink to the dump file created in the directory.


Note also the file 'config-geolocate'.  This is normally sourced into the
various scripts, and sets a number of environment variables, including those
naming the dump file and the various intermediate data files produced.
This relies on the following environment variables:

WP_VERSION       Specifies which dump file to use, e.g. "enwiki-20111007".
USE_PERMUTED     If set to "no", uses the non-permuted version of the dump
                 file.  If set to "yes", always try to use the permuted
                 version.  If blank or unset, use permuted version if it
                 appears to exist, non-permuted otherwise.

These two variables can be set on the command line used to execute
'run-processwiki'.


------ In even more detail ------

The following describes in detail the set of steps executed by
'download-preprocess-wiki' to generate a corpus.  It shows how individual
steps could be rerun if needed.  We assume that we are working with the
dump named "enwiki-20111007".

1. A new directory 'enwiki-20111007' is created to hold the downloaded dump
   file and all the generated files.  The dump is downloaded into this
   directory as follows:

wget -nd http://dumps.wikimedia.org/enwiki/20111007/enwiki-20111007-pages-articles.xml.bz2

   In all future steps, we work with this new directory as the current
   directory.

2. The basic article data file 'enwiki-20111007-document-data.txt' is
   created, giving metadata on the articles in the raw dump file
   'enwiki-20111007-pages-articles.xml.bz2', one article per line:

USE_PERMUTED=no run-processwiki article-data

   Note that, in this and future invocations of 'run-processwiki', we set
   the environment variable WP_VERSION to 'enwiki-20111007' to tell it
   the prefix of the dump file and the generated output files.

3. A permuted dump file 'enwiki-20111007-permuted-pages-articles.xml.bz2' is
   generated.  This uses the article data file from the previous step to
   specify the list of articles.  Basically, this list is permuted randomly,
   then divided into 8 parts according to the new order.  Then, the dump file
   is processed serially article-by-article, outputting the article into one
   of the 8 split files, according to the part it belongs in.  Then, each
   split file is sorted in memory and written out again, and finally the
   parts are concatenated and compressed.  This avoids having to read the
   entire dump file into memory and sort it, which may not be possible.

run-permute all

4. Split the permuted dump into 8 parts, for faster processing.  This could
   probably re-use the parts from the previous step, but currently it is
   written to be an independent operation.

run-processwiki split-dump

   Note also that from here on out, we set USE_PERMUTED to 'yes' to make
   sure that we use the permuted dump file. ('run-processwiki' attempts to
   auto-detect whether to use the permuted file if USE_PERMUTED isn't set,
   but it's safer to specify this explicitly.)

5. Generate a combined article data file from the permuted dump file.

run-processwiki combined-article-data

Note that from here on out, we set NUM_SIMULTANEOUS to 8 to indicate that
we should do parallel operation on the 8 split parts.  This will spawn off
8 processes to handle the separate parts, except in certain cases where
the parts cannot be processed separately.

This step actually has a number of subparts:

   a. Generate 'enwiki-20111007-permuted-article-data.txt', a basic article
      data file for the permuted dump.  This is like the article data file
      generated above for the raw, non-permuted dump.  It has one line of
      metadata for each article, with a simple database-like format with
      fields separated by tab characters, where the first line specifies the
      name of each field.

   b. Generate 'enwiki-20111007-permuted-coords.txt', specifying the
      coordinates (geotags) of articles in the dump file, when such
      information can be extracted.  Extracting the information involves a
      large amount of tricky and tedious parsing of the articles.

   c. Generate 'enwiki-20111007-permuted-links-only-coord-documents.txt',
      listing, for all articles with coordinates, the count of incoming
      internal links pointing to them from some other article.  Also contains,
      for each such article,  a list of all the anchor texts used when pointing
      to that article, along with associated counts. (This additional
      information is not currently used.)

   d. Generate 'enwiki-20111007-permuted-combined-document-data.txt'.
      This combines the information from the previous three steps.  This
      file has the same format as the basic article data file, but has two
      additional fields, specifying the coordinates and incoming link count.
      Unlike the basic article data file, it only lists articles with
      coordinates.

6. Generate 'enwiki-20111007-permuted-counts-only-coord-documents.txt',
   the word-counts file for articles with coordinates.  This is the remaining
   info needed for creating a TextGrounder corpus.

run-processwiki coord-counts

7. Generate some additional files, not currently needed:

   a. 'enwiki-20111007-permuted-counts-all-documents.txt': Word counts for
      all articles, not just those with coordinates.

   b. 'enwiki-20111007-permuted-text-only-coord-documents.txt': The actual
      text, pre-tokenized into words, of articles with coordinates.

   c. 'enwiki-20111007-permuted-text-all-documents.txt': The actual text,
      pre-tokenized into words, of all articles.

8. Remove the various intermediate split files created by the above steps.

9. Generate the corpus files.  This involves combining the metadata in
   'enwiki-20111007-permuted-combined-document-data.txt' with the word-count
   information in 'enwiki-20111007-permuted-counts-only-coord-documents.txt'
   into a combined file with one line per document; splitting this data into
   three files, one each for the training, dev and test sets; and creating
   corresponding schema files.

   This is done as follows:

convert-corpus-to-latest enwiki-20111007


------ How to rerun a single step ------

If all the preprocessing has already been done for you, and you simply want
to run a single step, then you don't need to do all of the above steps.
However, it's still strongly recommended that you do your work in a fresh
directory, and symlink the dump file into that directory -- in this case the
*permuted* dump file.  We use the permuted dump file for experiments because
the raw dump file has a non-uniform distribution of articles, and so we can't
e.g. count on our splits being uniformly distributed.  Randomly permuting
the dump file and article lists takes care of that.  The permuted dump file
has a name like

enwiki-20111007-permuted-pages-articles.xml.bz2

For example, if want to change processwiki.py to generate bigrams, and then
run it to generate the bigram counts, you might do this:

1. Note that there are currently options `output-coord-counts` to output
   unigram counts only for articles with coordinates (which are the only ones
   needed for standard document geotagging), and `output-all-counts` to
   output unigram counts for all articles.  You want to add corresponding
   options for bigram counts -- either something like
   `output-coord-bigram-counts` and `output-all-bigram-counts`, or an option
   `--n-gram` to specify the N-gram size (1 for unigrams, 2 for bigrams,
   3 for trigrams if that's implemented, etc.).  *DO NOT* in any circumstance
   simply hack the code so that it automatically outputs bigrams instead of
   unigrams -- such code CANNOT be incorporated into the repository, which
   means your mods will become orphaned and unavailable for anyone else.

2. Modify 'config-geolocate' so that it has additional sets of environment
   variables for bigram counts.  For example, after these lines:

COORD_COUNTS_SUFFIX="counts-only-coord-documents.txt"
ALL_COUNTS_SUFFIX="counts-all-documents.txt"

   you'd add

COORD_BIGRAM_COUNTS_SUFFIX="bigram-counts-only-coord-documents.txt"
ALL_BIGRAM_COUNTS_SUFFIX="bigram-counts-all-documents.txt"

   Similarly, after these lines:

OUT_COORD_COUNTS_FILE="$DUMP_PREFIX-$COORD_COUNTS_SUFFIX"
OUT_ALL_COUNTS_FILE="$DUMP_PREFIX-$ALL_COUNTS_SUFFIX"

   you'd add

OUT_COORD_BIGRAM_COUNTS_FILE="$DUMP_PREFIX-$COORD_BIGRAM_COUNTS_SUFFIX"
OUT_ALL_BIGRAM_COUNTS_FILE="$DUMP_PREFIX-$ALL_BIGRAM_COUNTS_SUFFIX"

   And then you'd do the same thing for IN_COORD_COUNTS_FILE and
   IN_ALL_COUNTS_FILE.

3. Modify 'run-processwiki', adding new targets ("steps")
   'coord-bigram-counts' and 'all-bigram-counts'.  Here, you would just
   copy the existing lines for 'coord-counts' and 'all-counts' and modify
   them appropriately.

4. Now finally you can run it:

WP_VERSION=enwiki-20111007 run-processwiki coord-bigram-counts

   This generates the bigram counts for geotagged articles -- the minimum
   necessary for document geotagging.

   Actually, since the above might take awhile and generate a fair amount
   of diagnostic input, you might want to run it in the background
   under nohup, so that it won't die if your terminal connection suddenly
   dies.  One way to do that is to use the TextGrounder 'run-nohup' script:

WP_VERSION=enwiki-20111007 run-nohup --id do-coord-bigram-counts run-processwiki coord-bigram-counts

   Note that the '--id do-coord-bigram-counts' is optional; all it does is
   insert the text "do-coord-bigram-counts" into the file that it stores
   stdout and stderr output into.  This file will have a name beginning
   'run-nohup.' and ending with a timestamp.  The beginning and ending of the
   file will indicate the starting and ending times, so you can see how long
   it took.

   If you want to generate bigram counts for all articles, you could use a
   similar command line, although it might take a couple of days to complete.
   If you're on Longhorn, where you only have 24-hour time slots, you might
   consider using the "divide-and-conquer" mode.  The first thing is to
   split the dump file, like this:

WP_VERSION=enwiki-20111007 run-processwiki split-dump

   This takes maybe 45 mins and splits the whole dump file into 8 pieces.
   (Controllable through NUM_SPLITS.)

   Then, each operation you want to do in divide-and-conquer mode, run it
   by setting NUM_SIMULTANEOUS to something more than 1, e.g.

WP_VERSION=enwiki-20111007 NUM_SIMULTANEOUS=8 run-processwiki all-bigram-counts

   (although you probably want to wrap it in 'run-nohup').  Essentially,
   this runs 8 simultaneous run-processwiki processes (which fits well with
   the workhorse Longhorn machines, since they are 8-core), one on each of
   the 8 splits, and then concatenates the results together at the end.
   You can set a NUM_SIMULTANEOUS that's lower than the number of splits,
   and you get only that much simultaneity.


------ Input files, intermediate files, output files ------

These are specified from the point of view of a dump named "enwiki-20111007",
in approximate order of generation.

enwiki-20111007-pages-articles.xml.bz2
  Raw dump of article text, downloaded from
  http://download.wikimedia.org/enwiki/20111007/


, specifying coordinates (geotags) of articles in the dump file,
  when such information can be extracted.  Extracting the information
  involves a large amount of tricky and tedious parsing of the articles.

enwiki-20111007-coords-counts.txt
  -- Output from passing raw dump through processwiki.py --coords-counts
  -- For all articles with attached coordinates, lists the article name,
     coordinates, and counts of all words seen as part of the "useful text"
     (filtering out directives and such; see processwiki.py for more info)

enwiki-20111007-just-titles-coords.txt
  -- Result of filtering output of --coords-counts for only the article
     title and coordinates, with word counts omitted.

enwiki-20111007-links-only-coord-articles.txt
  -- Result of running the following:

     processwiki.py --find-links \
       --coords-file enwiki-20111007-just-titles-coords.out

  -- For each article with coordinates, lists the number of incoming links
     to that article.  In addition, a set of tables listing surface forms of
     links and the corresponding articles linked to, with counts, is output.
     The surface form of a link is the text that the user sees.  Basically,
     the entire set of links pointing to articles with coordinates is grouped
     according to the surface form of the link, and for each different surface
     form, a table is output listing all articles linked to and the
     corresponding counts.  This data might be useful in constructing a
     disambiguator for mapping surface forms to canonical forms for geographic
     entities (i.e. mapping toponyms to locations).


Other data that could be useful:

-- Map of all redirects (e.g. "U.S." -> United States)
-- Map of all disambiguation pages and the articles linked to
-- The inverse surface->article mapping, i.e. for each article, list of
   surface forms that link to the article, along with counts
-- List of all pairs of articles that mutually link to each other, along with
   counts: Useful for indicating closely linked articles
-- Tables of categories (constructed from articles with a [[Category:foo]]
   link in them) and lists (construct from "List of foo" articles)
